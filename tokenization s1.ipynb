{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:23.536442900Z",
     "start_time": "2026-02-09T13:18:23.123580100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from transformers import BertTokenizer\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from datetime import datetime\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "b5b1fbdb7f3194a0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:23.598370900Z",
     "start_time": "2026-02-09T13:18:23.551453300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text=\"this is a sample sentence for word tokenization\"\n",
    "tokens=word_tokenize(text)\n",
    "print(tokens)"
   ],
   "id": "1bce68730c2dd529",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sample', 'sentence', 'for', 'word', 'tokenization']\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:23.643125800Z",
     "start_time": "2026-02-09T13:18:23.599396100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text=(\"I coldn't help the dog. Can't you do it?Dont't be afraid if you are.\")\n",
    "tokens=word_tokenize(text)\n",
    "print(tokens)"
   ],
   "id": "2315b77ecb193d4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'cold', \"n't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', 'it', '?', \"Dont't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:24.219472900Z",
     "start_time": "2026-02-09T13:18:23.647333800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text=\"I coldn't help the dog. Can't you do it?Don't be afraid if you are.\"\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "doc=nlp(text)\n",
    "\n",
    "token_list=[token.text for token in doc]\n",
    "print(token_list)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_,token.dep_)"
   ],
   "id": "23663bdbddbd623e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"coldn't\", 'help', 'the', 'dog', '.', 'Ca', \"n't\", 'you', 'do', \"it?Don't\", 'be', 'afraid', 'if', 'you', 'are', '.']\n",
      "I PRON nsubj\n",
      "coldn't AUX aux\n",
      "help VERB ROOT\n",
      "the DET det\n",
      "dog NOUN dobj\n",
      ". PUNCT punct\n",
      "Ca AUX aux\n",
      "n't PART neg\n",
      "you PRON nsubj\n",
      "do AUX aux\n",
      "it?Don't ADV neg\n",
      "be AUX ROOT\n",
      "afraid ADJ acomp\n",
      "if SCONJ mark\n",
      "you PRON nsubj\n",
      "are AUX advcl\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:24.291879200Z",
     "start_time": "2026-02-09T13:18:24.241131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text=\"Unicorn are real. I saw a unicorn yesterday\"\n",
    "token=word_tokenize(text)\n",
    "print(token)"
   ],
   "id": "593a42a7c2669c6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unicorn', 'are', 'real', '.', 'I', 'saw', 'a', 'unicorn', 'yesterday']\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:25.704057500Z",
     "start_time": "2026-02-09T13:18:24.303916800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ],
   "id": "8679cd7b2c2379c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ibm', 'taught', 'me', 'token', '##ization', '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:28.708419Z",
     "start_time": "2026-02-09T13:18:25.736547800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ],
   "id": "bb85acbbc919b85e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁IBM', '▁taught', '▁me', '▁token', 'ization', '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:28.749583400Z",
     "start_time": "2026-02-09T13:18:28.733534300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
    "    (1,\" Machine Translation with NLP \"),\n",
    "    (1,\" Named Entity vs Sentiment Analysis  NLP \")]"
   ],
   "id": "a8158f678d1615d2",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:28.765831700Z",
     "start_time": "2026-02-09T13:18:28.752330300Z"
    }
   },
   "cell_type": "code",
   "source": "from torchtext.data.utils import get_tokenizer",
   "id": "9dbf165c4103d28b",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:28.804954800Z",
     "start_time": "2026-02-09T13:18:28.767832Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer=get_tokenizer(\"basic_english\")",
   "id": "8f13b3f3d0922181",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:28.832681700Z",
     "start_time": "2026-02-09T13:18:28.805954900Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer(dataset[0][1])",
   "id": "e1382560855579bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction', 'to', 'nlp']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:28.857792800Z",
     "start_time": "2026-02-09T13:18:28.834680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _,text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "my_iterator=yield_tokens(dataset)"
   ],
   "id": "df81174648661016",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:28.882703700Z",
     "start_time": "2026-02-09T13:18:28.858795200Z"
    }
   },
   "cell_type": "code",
   "source": "next(my_iterator)",
   "id": "53fd5ca1c3921f45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction', 'to', 'nlp']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:28.967992200Z",
     "start_time": "2026-02-09T13:18:28.883989700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab=build_vocab_from_iterator(yield_tokens(dataset),specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ],
   "id": "353aa2d9a8409edf",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:29.013563300Z",
     "start_time": "2026-02-09T13:18:28.970994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_tokenized_sentence_and_indices(iterator):\n",
    "    tokenized_sentence=next(iterator)\n",
    "    token_indices=[vocab[token] for token in tokenized_sentence]\n",
    "    return tokenized_sentence,token_indices\n",
    "tokenized_sentence,token_indices=get_tokenized_sentence_and_indices(my_iterator)\n",
    "next(my_iterator)\n",
    "\n",
    "print(tokenized_sentence)\n",
    "print(token_indices)"
   ],
   "id": "9888020cc1ea16d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['basics', 'of', 'pytorch']\n",
      "[11, 15, 2]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:29.634928800Z",
     "start_time": "2026-02-09T13:18:29.015956700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lines=[\"IBM taught me tokenization.\",\"special tokenizer are ready and they will blow your mind\",\"just say hi\"]\n",
    "special_symobols=[\"<unk>\",'<pad>','<bos>','<eos>']\n",
    "tokenizer_en=get_tokenizer('spacy',language='en')\n",
    "tokens=[]\n",
    "max_length=0\n",
    "for line in lines:\n",
    "    tokenized_line=tokenizer_en(line)\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i]=tokens[i]+['<pad>']*(max_length-len(tokens[i]))\n",
    "print(tokens)\n",
    "\n",
    "vocab=build_vocab_from_iterator(tokens,specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "print(vocab.get_itos())\n",
    "print(vocab.get_stoi())"
   ],
   "id": "7ff04442904f2d52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<bos>', 'IBM', 'taught', 'me', 'tokenization', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'special', 'tokenizer', 'are', 'ready', 'and', 'they', 'will', 'blow', 'your', 'mind', '<eos>'], ['<bos>', 'just', 'say', 'hi', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n",
      "['<unk>', '<pad>', '<bos>', '<eos>', '.', 'IBM', 'and', 'are', 'blow', 'hi', 'just', 'me', 'mind', 'ready', 'say', 'special', 'taught', 'they', 'tokenization', 'tokenizer', 'will', 'your']\n",
      "{'.': 4, '<bos>': 2, 'blow': 8, '<unk>': 0, 'and': 6, '<eos>': 3, '<pad>': 1, 'will': 20, 'IBM': 5, 'are': 7, 'hi': 9, 'just': 10, 'me': 11, 'mind': 12, 'ready': 13, 'say': 14, 'special': 15, 'taught': 16, 'they': 17, 'your': 21, 'tokenization': 18, 'tokenizer': 19}\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:29.699798400Z",
     "start_time": "2026-02-09T13:18:29.655538800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_lines=\"I learned about embedding and attention mechanisms\"\n",
    "tokenized_new_line=tokenizer_en(new_lines)\n",
    "tokenized_new_line=[\"<bos>\"]+tokenized_new_line+['<eos>']\n",
    "new_lines_padded=tokenized_new_line+['<pad>']*(max_length - len(tokenized_new_line))\n",
    "new_lines_ids=[vocab[token] if token in vocab else vocab['<unk>'] for token in new_lines_padded]\n",
    "print(new_lines_ids)"
   ],
   "id": "6fdc9489c7798da7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 0, 0, 0, 6, 0, 0, 3, 1, 1, 1]\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:18:29.748795900Z",
     "start_time": "2026-02-09T13:18:29.702914800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "Going through the world of tokenization has been like walking through a huge maze made of words, symbols, and meanings. Each turn shows a bit more about the cool ways computers learn to understand our language. And while I'm still finding my way through it, the journey’s been enlightening and, honestly, a bunch of fun.\n",
    "Eager to see where this learning path takes me next!\"\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "def show_frequencies(tokens,method_name):\n",
    "    print(f\"{method_name}token frequencies:{dict(Counter(tokens))}\\n\")\n",
    "\n"
   ],
   "id": "353b1da41da67d1",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:23:21.334538900Z",
     "start_time": "2026-02-09T13:23:16.118153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_time=datetime.now()\n",
    "nltk_tokens=nltk.word_tokenize(text)\n",
    "nltk_time=datetime.now()-start_time\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "start_time=datetime.now()\n",
    "spacy_tokens=[token.text for token in nlp(text)]\n",
    "spacy_time=datetime.now()-start_time\n",
    "\n",
    "bert_tokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "start_time=datetime.now()\n",
    "bert_tokens=bert_tokenizer.tokenize(text)\n",
    "bert_time=datetime.now()-start_time\n",
    "\n",
    "xlnet_tokenizer=XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "start_time=datetime.now()\n",
    "xlnet_tokens=xlnet_tokenizer.tokenize(text)\n",
    "xlnet_time=datetime.now()-start_time\n",
    "\n",
    "print(f\"NLTK Tokens: {nltk_tokens}\\nTime Taken: {nltk_time} seconds\\n\")\n",
    "show_frequencies(nltk_tokens, \"NLTK\")\n",
    "\n",
    "print(f\"SpaCy Tokens: {spacy_tokens}\\nTime Taken: {spacy_time} seconds\\n\")\n",
    "show_frequencies(spacy_tokens, \"SpaCy\")\n",
    "\n",
    "print(f\"Bert Tokens: {bert_tokens}\\nTime Taken: {bert_time} seconds\\n\")\n",
    "show_frequencies(bert_tokens, \"Bert\")\n",
    "\n",
    "print(f\"XLNet Tokens: {xlnet_tokens}\\nTime Taken: {xlnet_time} seconds\\n\")\n",
    "show_frequencies(xlnet_tokens, \"XLNet\")"
   ],
   "id": "7ce3429444542344",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Tokens: ['Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’', 's', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', \"''\"]\n",
      "Time Taken: 0:00:00.000528 seconds\n",
      "\n",
      "NLTKtoken frequencies:{'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’': 1, 's': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, \"''\": 1}\n",
      "\n",
      "SpaCy Tokens: ['\\n', 'Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’s', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', '\\n', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"', '\\n']\n",
      "Time Taken: 0:00:00.035114 seconds\n",
      "\n",
      "SpaCytoken frequencies:{'\\n': 3, 'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’s': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
      "\n",
      "Bert Tokens: ['going', 'through', 'the', 'world', 'of', 'token', '##ization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'and', 'while', 'i', \"'\", 'm', 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’', 's', 'been', 'en', '##light', '##ening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"']\n",
      "Time Taken: 0:00:00.006078 seconds\n",
      "\n",
      "Berttoken frequencies:{'going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'token': 1, '##ization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 3, 'meanings': 1, '.': 3, 'each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'while': 1, 'i': 1, \"'\": 1, 'm': 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’': 1, 's': 1, 'en': 1, '##light': 1, '##ening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
      "\n",
      "XLNet Tokens: ['▁Going', '▁through', '▁the', '▁world', '▁of', '▁token', 'ization', '▁has', '▁been', '▁like', '▁walking', '▁through', '▁a', '▁huge', '▁maze', '▁made', '▁of', '▁words', ',', '▁symbols', ',', '▁and', '▁meaning', 's', '.', '▁Each', '▁turn', '▁shows', '▁a', '▁bit', '▁more', '▁about', '▁the', '▁cool', '▁ways', '▁computers', '▁learn', '▁to', '▁understand', '▁our', '▁language', '.', '▁And', '▁while', '▁I', \"'\", 'm', '▁still', '▁finding', '▁my', '▁way', '▁through', '▁it', ',', '▁the', '▁journey', '’', 's', '▁been', '▁enlighten', 'ing', '▁and', ',', '▁honestly', ',', '▁a', '▁bunch', '▁of', '▁fun', '.', '▁E', 'ager', '▁to', '▁see', '▁where', '▁this', '▁learning', '▁path', '▁takes', '▁me', '▁next', '!', '\"']\n",
      "Time Taken: 0:00:00.004835 seconds\n",
      "\n",
      "XLNettoken frequencies:{'▁Going': 1, '▁through': 3, '▁the': 3, '▁world': 1, '▁of': 3, '▁token': 1, 'ization': 1, '▁has': 1, '▁been': 2, '▁like': 1, '▁walking': 1, '▁a': 3, '▁huge': 1, '▁maze': 1, '▁made': 1, '▁words': 1, ',': 5, '▁symbols': 1, '▁and': 2, '▁meaning': 1, 's': 2, '.': 3, '▁Each': 1, '▁turn': 1, '▁shows': 1, '▁bit': 1, '▁more': 1, '▁about': 1, '▁cool': 1, '▁ways': 1, '▁computers': 1, '▁learn': 1, '▁to': 2, '▁understand': 1, '▁our': 1, '▁language': 1, '▁And': 1, '▁while': 1, '▁I': 1, \"'\": 1, 'm': 1, '▁still': 1, '▁finding': 1, '▁my': 1, '▁way': 1, '▁it': 1, '▁journey': 1, '’': 1, '▁enlighten': 1, 'ing': 1, '▁honestly': 1, '▁bunch': 1, '▁fun': 1, '▁E': 1, 'ager': 1, '▁see': 1, '▁where': 1, '▁this': 1, '▁learning': 1, '▁path': 1, '▁takes': 1, '▁me': 1, '▁next': 1, '!': 1, '\"': 1}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
